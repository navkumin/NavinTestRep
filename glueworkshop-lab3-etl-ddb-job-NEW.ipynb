{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "glue_pyspark"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Adding required libraries and extra jars to the job -   # <------- PLEASE REPLACE ${BUCKET_NAME} BELOW!!!\n\n%extra_py_files s3://glueworkshop-100022632938-us-east-1/library/pycountry_convert.zip\n%extra_jars s3://crawler-public/json/serde/json-serde.jar\n\n# Adding required properties to the job - # <------- PLEASE REPLACE ${BUCKET_NAME} BELOW!!!\n\n%%configure \n{\n  \"--enable-spark-ui\": \"true\",\n  \"--spark-event-logs-path\": \"s3://glueworkshop-100022632938-us-east-1/output/lab3/sparklog/\",\n  \"max_retries\": \"0\"         \n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nExtra py files to be included:\ns3://glueworkshop-100022632938-us-east-1/library/pycountry_convert.zip\nExtra jars to be included:\ns3://crawler-public/json/serde/json-serde.jar\nThe following configurations have been updated: {'--enable-spark-ui': 'true', '--spark-event-logs-path': 's3://glueworkshop-100022632938-us-east-1/output/lab3/sparklog/', 'max_retries': '0'}\ns3://crawler-public/json/serde/json-serde.jar\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Importing all the basic Glue, Spark libraries \n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n# Important further required libraries\n\nimport os, sys, boto3\nfrom pprint import pprint\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import IntegerType, StringType\nfrom datetime import datetime\n\n# Starting Spark/Glue Context\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n# Important pycountry_convert function from the external python library (pycountry_convert.zip)\n\nfrom pycountry_convert import (\n    convert_country_alpha2_to_country_name,\n    convert_country_alpha2_to_continent,\n    convert_country_name_to_country_alpha2,\n    convert_country_alpha3_to_country_alpha2,\n)\n\n# Defining the function code\n\ndef get_country_code2(country_name):\n    country_code2 = 'US'\n    try:\n        country_code2 = convert_country_name_to_country_alpha2(country_name)\n    except KeyError:\n        country_code2 = ''\n    return country_code2\n\nudf_get_country_code2 = udf(lambda z: get_country_code2(z), StringType())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: 8d865432-c018-4215-9975-be13f0f5e55b\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--enable-spark-ui true\n--spark-event-logs-path s3://glueworkshop-100022632938-us-east-1/output/lab3/sparklog/\n--max_retries 0\n--extra-py-files s3://glueworkshop-100022632938-us-east-1/library/pycountry_convert.zip\n--extra-jars s3://crawler-public/json/serde/json-serde.jar\nWaiting for session 8d865432-c018-4215-9975-be13f0f5e55b to get into ready status...\nSession 8d865432-c018-4215-9975-be13f0f5e55b has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Get parameter values\n\ns3_bucket_name = \"s3://glueworkshop-100022632938-us-east-1/\"                              # <------- PLEASE REPLACE ONLY THE ${BUCKET_NAME} HERE (Keep the \"s3://\" and the final \"/\" part)!!!\nregion_name = 'us-east-1'                                        #  <--- REPLACE THE AWS REGION\nddb_table_name='glueworkshop-lab3-new'\n\n\n# Create the dynamodb with appropriate read and write capacity\n# Get service resource\ndynamodb = boto3.resource('dynamodb', region_name=region_name)\n\ntable_status = dynamodb.create_table(\n    TableName=ddb_table_name,\n    KeySchema=[{'AttributeName': 'uuid','KeyType': 'HASH'}],\n    AttributeDefinitions=[{'AttributeName': 'uuid','AttributeType': 'N'}],\n    ProvisionedThroughput={'ReadCapacityUnits': 500,'WriteCapacityUnits': 5000}\n    )\n# Wait until the table exists.\ntable_status.meta.client.get_waiter('table_exists').wait(TableName=ddb_table_name)\npprint(table_status)\n\ndf = spark.read.load(s3_bucket_name + \"input/lab2/sample.csv\", \n                     format=\"csv\", \n                     sep=\",\", \n                     inferSchema=\"true\", \n                     header=\"true\")\n\n\nnew_df = df.withColumn('country_code_2', udf_get_country_code2(col(\"Country\")))\nnew_df_dyf=DynamicFrame.fromDF(new_df, glueContext, \"new_df_dyf\")\n\nprint(\"Start writing to DBB : {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\nglueContext.write_dynamic_frame_from_options(\n    frame=new_df_dyf,\n    connection_type=\"dynamodb\",\n    connection_options={\n        \"dynamodb.output.tableName\": ddb_table_name,\n        \"dynamodb.throughput.write.percent\": \"1.0\"\n    }\n)\nprint(\"Finished writing to DBB : {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n\n# Comparing Counts\n    \nnew_df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "dynamodb.Table(name='glueworkshop-lab3-new')\nStart writing to DBB : 2025-01-30 22:21:27\nFinished writing to DBB : 2025-01-30 22:21:53\n100000\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}